# Atari_DQN
Learning based on the deep Q-network

## Воспроизведение результатов DQN на играх Atari.

* Некоторые игры могут потребовать специфических методов(пример - Montezuma's Revenge, etc)
* Используются техники из статьи DeepMind: replay buffer, frame skip, target network.
* Во многих случаях табличные методы обучения с подкреплением не справляются с пространством действий и состояний.
* Для обобщения значений функции полезности на ненаблюдаемые состояния используется аппроксимация
* Идея заключается в том, чтобы использовать память прецедентов (replay buffer).
 
## Визуализация

[https://wandb.ai/senich17/DQN_Atari?nw=nwusersenich17]

## Выводы по гиперпараметрам

### Learning rate

* Высокое значение:
Быстрая сходимость: Модель может быстро сходиться в начале обучения, что полезно, когда нужно быстро получить работающую модель.
Нестабильность: Может возникнуть нестабильность в процессе обучения, проявляющаяся в виде колебаний или даже расходжения функции потерь.
Пропуск оптимальных решений: Слишком большие шаги могут привести к тому, что модель будет "перепрыгивать" через оптимальные точки.

* Низкий learning_rate:
Стабильность: Обеспечивает более стабильное обучение, с меньшими колебаниями в процессе.
Медленная сходимость: Может существенно замедлить процесс обучения, что неэффективно в условиях ограниченных ресурсов.
Риск застревания в локальных минимумах: При очень низком learning_rate существует риск, что модель застрянет в локальном минимуме и не достигнет лучшего решения.

### Buffer size

* Небольшой buffer_size:
Быстрое обучение: Агент быстрее обучается, так как обучение происходит на меньшем количестве данных, что требует меньше времени на обработку.
Риск переобучения: Агент может начать переобучаться на часто встречающихся примерах в буфере, что ухудшит его способность генерализовать поведение в новых или редко встречающихся ситуациях.
Нестабильность обучения: Обучение может стать более нестабильным, если опыт в буфере не достаточно представителен для всего пространства состояний.

* Большoй buffer_size:
Стабильность обучения: Обучение становится более стабильным за счет использования более разнообразного и представительного набора данных.
Уменьшение переобучения: Больший буфер снижает вероятность переобучения, поскольку агент использует более широкий и разнообразный набор опытов для обучения.
Увеличение времени обучения: Обработка большего количества данных может увеличить время, необходимое для обучения, поскольку каждый пакет данных требует больше времени для обработки.

### Дополнительно

[https://colab.research.google.com/drive/14-laEI5JlzRCThZ1U9Y-_uxURCQj3tot?usp=sharing]
